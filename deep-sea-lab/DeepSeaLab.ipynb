{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeaAI\n",
    "#### Pipeline used for the cleaning and analysis of citizen science data with AI.\n",
    "\n",
    "This markdown will explain how to clean your citizen science data, and train a Yolov8 model on it. It has been developed with the intent to deal with DeepSeaSpy data (accessible from :https://zenodo.org/records/13759095)\n",
    "https://ocean-spy.ifremer.fr/\n",
    "\n",
    "Note : some functions may copy your images (specifically **vision**, **catalog**, **prepare_yolo**). Please be wary of the space available on your computer/deployment/cloud storage and act accordingly.\n",
    "\n",
    "### Contents\n",
    "You should have a Jupyter notebook file and a python file, both necessary :\n",
    "\n",
    "```\n",
    "deep-sea-lab\n",
    "├── DeepSeaLab.ipynb        <- You are here\n",
    "├── Functions.py            <- Where functions used for the cleaning/analysis are stored.\n",
    "```\n",
    "\n",
    "**Functions** contains all the functions needed for the cleaning and analysis of citizen science datasets. Detailed explanations of the functions are to be found here. You can modify them and use them as the basis for your work. There are also functions that are useful just for the exploration of your dataset, and advanced specific modifications.\n",
    "\n",
    "**DeepSeaLab** is a ready-to-use file that you can change based on your needs/dataset. Explanations are given on how to proceed and guide you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: sys.version_info(major=3, minor=10, micro=16, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"Python version:\", sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "Import the necessaries functions for the cleaning/analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\imagine\\\\github\\\\deep-species-detection\\\\deep-sea-lab'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, csv, json, collections, random, shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from Functions import polygones2bb, points2bb, lines2bb, convert_yolo, prepare_yolo, vision, SaveCSV, create_yaml\n",
    "from Functions import unite, catalog, get_df\n",
    "import matplotlib.pyplot as plt\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access to data/files\n",
    "3 main paths are to be defined :\n",
    "\n",
    "```\n",
    "path_csv\n",
    "```\n",
    "Location of your dataset.\n",
    "\n",
    "\n",
    "```\n",
    "path_img\n",
    "```\n",
    "Path to the folder where your images are stored.\n",
    "The format of the images should be in .jpg or .png\n",
    "\n",
    "```\n",
    "images\n",
    "├── image 1.jpg           \n",
    "├── image 2.jpg          \n",
    "├── ... \n",
    "```\n",
    "\n",
    "Then, finally\n",
    "```\n",
    "path_save\n",
    "```\n",
    "Where to store the cleaned dataset, catalogs, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv access :\n",
    "path_csv=r'/storage/export.csv'\n",
    "# images :\n",
    "path_img=Path(r'/storage/Image_dsp/') \n",
    "# save\n",
    "path_save=r'/storage/save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion of bounding boxes\n",
    "\n",
    "Our pipeline can convert 3 type of bounding boxes into regular bounding boxes.\n",
    "If your data already satisfies the following format, you can skip this part.\n",
    "\n",
    "|xmin |ymin |xmax |ymax |\n",
    "|-----|-----|-----|-----|\n",
    "|972  |982  |549  |559  |\n",
    "\n",
    "\n",
    "#### Polygons\n",
    "On DeepSeaSpy, polygons are in the json format :\n",
    "\n",
    "```\n",
    "[{\\x\\\":282,\\\"y\\\":115},{\\\"x\\\":15,\\\"y\\\":538},{\\\"x\\\":50,\\\"y\\\":679},{\\\"x\\\":285,\\\"y\\\":497}]\n",
    "```\n",
    "\n",
    "The column containing the polygon values can be named :\n",
    "|polygon_values                                                                         |\n",
    "|---------------------------------------------------------------------------------------|\n",
    "|[{\\x\\\":282,\\\"y\\\":115},{\\\"x\\\":15,\\\"y\\\":538},{\\\"x\\\":50,\\\"y\\\":679},{\\\"x\\\":285,\\\"y\\\":497}] |\n",
    "\n",
    "Therefore our pipeline was made to deal with such format. If you need to convert your own type of polygons, you can modify the way points are stored in the **polygons2bb** function.\n",
    "\n",
    "#### Lines\n",
    "Lines are to be in the following format, with two points defined by (x1,y1) and (x2,y2).\n",
    "\n",
    "|x1 |y1 |x2 |y2 |length|\n",
    "|---|---|---|---|------|\n",
    "|761|451|859|364|131   |\n",
    "\n",
    "Length is used to correct the converted bounding box, depending on the line's angle with the x axis. If the line is too vertical or too horizontal, the lines2bb function automatically corrects the converted bounding box. By default, if the angle is of +-5 degrees, the corrections happens. You can modify/find mor info in the Functions.py file.\n",
    "\n",
    "#### Points\n",
    "You can manually set a padding on the x and y axis in the Functions.py file.\n",
    "\n",
    "|x1 |y1 |\n",
    "|---|---|\n",
    "|761|451|\n",
    "\n",
    "The padding is the same for every point in your dataset, if you wish to use a different one for different species/uses, we recommend you split your dataset and run each part with a different padding. Then, you can concatenate all of your subsets with :\n",
    "\n",
    "```\n",
    "pd.concat([polybb,lignesbb,pointsbb])\n",
    "```\n",
    "\n",
    "We alsor recommend changing the names of your images columns and species columns, so that our functions can run properly.\n",
    "\n",
    "|name_img         |name_sp     |\n",
    "|-----------------|------------|\n",
    "|'MOMAR_90095.jpg'|'Buccinidae'|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your dataset\n",
    "data=pd.read_csv(path_csv, sep=None, engine='python')\n",
    "data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename data imported from DeepSeaSpy\n",
    "#data.rename(columns={'pos1x': 'x1', 'pos1y': 'y1','pos2x': 'x2', 'pos2y': 'y2','name_fr':'name_sp','name':'name_img'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split your data according to your dataset. The conditions on which data are split was decided based on the DeepSeaSpy format.\n",
    "You can comment the lines of code that you don't want to be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset of polygon labels\n",
    "poly=data.dropna(subset=['polygon_values'])\n",
    "# Subset of lines labels\n",
    "lines=data.dropna(subset=['x2'])\n",
    "# Subset of points labels\n",
    "points=data[data['polygon_values'].isna() & data['x2'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polygons\n",
    "polybb=polygones2bb(poly)\n",
    "# Lines\n",
    "lignesbb=lines2bb(lines)\n",
    "# Points\n",
    "pointsbb=points2bb(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate your split dataset into a single one\n",
    "bb=pd.concat([polybb,lignesbb,pointsbb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the converted dataset\n",
    "SaveCSV(bb,path_save,'export_bb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb=pd.read_csv(os.path.join(path_save,'export_bb.csv'), sep=None, engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision\n",
    "\n",
    "We encourage you to use the vision function, which will allow you to visualize your images with your bounding boxes added onto them.\n",
    "First, you have to define the object 'colors', which is a dictonary containing each species with its corresponding color coded in BGR.\n",
    "\n",
    "If you wish to, you can limit the number of saved images by adding 'nb_img' as an argument. \n",
    "```\n",
    "vision(bb,colors,path_img,path_save=None,nb_img=None)\n",
    "```\n",
    "When not specified, it saves the images in the parent directory of the path_img.\n",
    "\n",
    "This function copies the images from path_img, so it may generate a lot of data if you don't specify a number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors are in BGR\n",
    "vision(bb, path_img, path_save, nb_img=10) #nb_img is optional, if you want to plot all of your data use \"nb_img=None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only color in red\n",
    "vision(bb, path_img, path_save, nb_img=10,colors='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unification of overlapping bounding boxes\n",
    "\n",
    "This step ensures that there is no redundancy in your dataset. You can skip this part if you are not dealing with this kind of problem.\n",
    "\n",
    "The unification of the bounding boxes is done when they are strictly overlapping (while the iou value is kept as None).\n",
    "Still, if you wish to limit the unification of the BB to a certain superposition threshold (iou), you can.\n",
    "\n",
    "```\n",
    "unite(dataframe, iou=None, grouper_0=False)\n",
    "```\n",
    "\n",
    "iou_thresh corresponds to the minimum Intersection over Union (IoU) value between two bounding boxes to consider them overlapping.\n",
    "\n",
    "Bounding boxes that are not overlapping any are automatically discarded. If you want to keep them, you can change the argument grouper_0 to grouper_0=True.\n",
    "\n",
    "The function keeps track of how many bounding boxes the final ones are made of in the column \"occurrences\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unification of all overlapping bounding boxes\n",
    "ubb=unite(bb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following boxes are examples of what you can do with the unite function. Try to experiment and find what may be the best parameters for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping bounding boxes only if they are made of at least 3 overlapping ones\n",
    "ubb=ubb[ubb['occurences']>=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unification only when bounding boxes are 0.2% overlapping\n",
    "ubb=unite(bb,0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unification all of your bounding boxes, while not discarding isolated ones\n",
    "ubb=unite(bb,grouper_0=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save your dataframe\n",
    "SaveCSV(ubb,path_save,'ubb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read your saved dataset\n",
    "ubb=pd.read_csv(os.path.join(path_save,'ubb.csv'), sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catalog\n",
    "\n",
    "Unite only unifies overlapping bounding boxes, it does not verify if the object you want to labelise is in fact inside the bounding box. If you wish to be very wary about which bounding box are to be kept in your dataset, you can use the two functions :\n",
    "\n",
    "```\n",
    "catalog(df, path_img, path_save=None)\n",
    "```\n",
    "When not specified, it saves the images in the parent directory of the path_img.\n",
    "\n",
    "Creates a catalog of snapshots from all the bounding boxes you have in your dataframe (df). You can then delete the snapshots of bounding boxes you want to discard.\n",
    "\n",
    "```\n",
    "get_df(df,path_save)\n",
    "```\n",
    "\n",
    "From the path_save (where your remaining snapshots are), and your unified dataframe (df), this function returns a dataframe that lists all the remaining bounding boxes from your own cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create snapshots from images and the dataframe\n",
    "catalog(ubb, path_img, path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After discarding snapshots, get the remaining rows\n",
    "ubb=get_df(ubb,path_save)\n",
    "ubb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Yolo\n",
    "\n",
    "You can then use prepare_yolo to split your dataset in 3 (train,val,test), and train yolov8 on it.\n",
    "\n",
    "```\n",
    "prepare_yolo(df,path_save,path_img,prop=[.8,.1])\n",
    "```\n",
    "\n",
    "The **prop** parameter stands for proportion. It asks for the size of 2 subsets (in order) : **train** and **validation**. The remaining percentage is the size of the **test** subset. The test subset is not mandatory for the training of a model.\n",
    "prop=[.8,.1] means that the training subsets is 80% of our dataset, the validation subset is 10%. The remaining percentage is the test subset's size, here, 10%.\n",
    "\n",
    "Yolov8 takes bounding boxes in the following format :\n",
    "\n",
    "```\n",
    "class x y w h\n",
    "```\n",
    "\n",
    "With x and y the coordinates to the center of the bounding box. W and h are the width and height of the bounding box. Those 4 values are normalised between 0 and 1. prepare_yolo generates 1 txt file for each image, and each line in this file is the description of 1 bounding box.\n",
    "\n",
    "example :\n",
    "```\n",
    "7 0.5713542 0.6847222 0.0359375 0.0731481\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_yolo(ubb,path_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_yolo(ubb,path_save,path_img,prop=[.8,.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yolo training\n",
    "Now you can train a yolo model with your dataset.\n",
    "\n",
    "Your yolo training folder path should look like this :\n",
    "\n",
    "```\n",
    "yolo_training\n",
    "├── images        <- Where your images are\n",
    "|   ├── train\n",
    "|   ├── val\n",
    "|   ├── test\n",
    "├── labels        <- Where your bounding boxes/labels for each image are\n",
    "|   ├── train\n",
    "|   ├── val\n",
    "|   ├── test\n",
    "```\n",
    "Yolo needs a yaml file to understands where the data is, and what the classes are.\n",
    "You can create the file yourself or use create_yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a yaml file containing all of the information necessary for running Yolov8 on your data\n",
    "create_yaml(ubb,path_save,'output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is ready for Yolov8.\n",
    "\n",
    "\n",
    "If you installed our requirements correctly, you can train your Yolov8 model in python :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "# Load a new YOLO model from scratch\n",
    "model = YOLO('yolov8n.yaml')  # build a new model from YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "# Or load a pretrained YOLO model (recommended for training)\n",
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get where the .yaml file is stored\n",
    "output=str('output'+'.yaml')\n",
    "yaml_path=os.path.join(path_save,output)\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data=yaml_path, epochs=1, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model to identify objects\n",
    "# Load your trained YOLOv8n model\n",
    "model = YOLO(os.path.join(os.getcwd(),r'runs\\detect\\train\\weights\\best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can then make a list of images to run inference on\n",
    "#Here, you can put them in a single folder\n",
    "img_to_predict=r'path\\to\\images\\to\\be\\predicted'\n",
    "list_img=list(img_to_predict.glob('**/*.jpg'))\n",
    "\n",
    "# Run inference on your list of images\n",
    "results = model.predict(list_img, save=True, save_txt=False, save_conf=False, show_conf=False, project=path_save, name='inference_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also launch the training within a command terminal (same line of code for windows or linux). This can be useful if you don't want to open a python interactive window, or you are working remotly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the training\n",
    "!yolo task=detect mode=train model=yolov8n.yaml imgsz=640 data=absolute/path/to/output.yaml show_labels=False epochs=10 batch=8 name=run1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once your model is trained, you can run 'predict' to detect objects\n",
    "!yolo predict model=path/to/yolo/runs/detect/run1/weights/best.pt source=path/to/data show_labels=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to go further\n",
    "\n",
    "If you wish to train the hyperparameters of your trained model, you can do so.\n",
    "This step is to be done only if you have the allowable resources to do so, as hyperparameter tuning takes a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('/runs/detect/buccin_cit_07/weights/best.pt')\n",
    "\n",
    "model.tune(data='output.yaml',epochs=200, iterations=300, optimizer='AdamW', plots=True, save=True, val=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
